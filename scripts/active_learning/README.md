# Use an active learning strategy to improve the ground truths

This README explains the steps that were performed to improve the quality of the ground truths segmentations used to train TotalSpineSeg.

## Original ground truths limitation
> For more information about the training, see https://www.researchgate.net/publication/389881289_TotalSpineSeg_Robust_Spine_Segmentation_with_Landmark-Based_Labeling_in_MRI 

Due to the time-consuming nature of manually segmenting the anatomical structures identified by TotalSpineSeg namely the spinal cord, spinal canal, vertebrae, and intervertebral discs, the [PAM50 atlas](https://pubmed.ncbi.nlm.nih.gov/29061527/) was registered to the native space of each scan that included the cervical and/or thoracic regions to serve as ground truth segmentation. However, because of inter-subject anatomical variability, the atlas did not perfectly align with every subject anatomy. These subtle misalignments introduced inconsistencies that propagated into the modelâ€™s predictions, often resulting in over- or under-segmentation of certain structures. This issue affects models trained up to and including release `r20250224`.

## Refinement of the existing ground truth (T2w)

These ground truth refinements will be done using existing contrasts specific deep learning models and manual corrections of the predictions.

### Canal segmentations

To improve the canal segmentations, the model [sc_canal_t2](https://spinalcordtoolbox.com/stable/user_section/command-line/deepseg/sc_canal_t2.html) available inside the spinalcord toolbox was used. The following steps present the process step by step:
> Note: This model is limited to T2w images

1. Install the [spinalcordtoolbox](https://github.com/spinalcordtoolbox/spinalcordtoolbox?tab=readme-ov-file#installation).
2. Install the canal segmentation model
```bash
sct_deepseg sc_canal_t2 -install
```
3. Run the model on niftii images
```bash
sct_deepseg sc_canal_t2 -i img.nii.gz -o canal.nii.gz
```

### Spine segmentations

To improve the vertebrae and discs segmentations, the model [spineps](https://github.com/Hendrik-code/spineps) was used. The following steps present the process step by step:
> Note: The release [1.3.0](https://github.com/Hendrik-code/spineps/releases/tag/v1.3.0) was used here.

1. Install [spineps](https://github.com/Hendrik-code/spineps?tab=readme-ov-file#installation-ubuntu)
2. Run the model on niftii images
```bash
spineps sample -ignore_bids_filter -ignore_inference_compatibility -i img.nii.gz -model_semantic t2w -model_instance instance -der_name spineps_folder
```

Because the version [1.3.0](https://github.com/Hendrik-code/spineps/releases/tag/v1.3.0) of SPINEPS only provides instance segmentations (vertebrae/discs are unlabeled). TotalSpineSeg `r20250224` was also used on the same images to have access to the labels,
and the canal segmentations were also used to remove segmented voxels of vertebrae and discs from the canal region. These last steps were performed using this last script.

```bash
python spineps_refine -spineps spineps_folder -totalspineseg step2_output -canal canal_folder -ofolder labels_refined
```
- `spineps_folder` is the output folder generated by SPINEPS
- `step2_output` is the output folder generated by totalspineseg
- `canal_folder` contains all the predictions generated by `sct_deepseg sc_canal_t2`
- `labels_refined` is the BIDS like output folder

## Active learning step

At this step TotalSpineSeg was retrained with the ground truth generated in the last section. The purpose of this step is to run TotalSpineSeg on:
- the training images to improve slight imperfections in the ground truths
- new images to incorporate more contrasts in the training

### Generating ground truth for new contrasts (T1w, MT-on, MT-off, T2star)

Thanks to the new nnUNetTrainerDAext (available under totalspineseg/trainer/nnUNetTrainerDAExt.py), TotalSpineSeg demonstrated strong performance in semantically segmenting structures (spinal cord, canal, vertebrae, discs) on contrasts that were unseen during initial training. However, since the training data mainly included sacral scans (due to the availability of manual segmentations), the model showed limitations in accurately detecting cervical landmarks, reducing its direct applicability. To overcome this limitation and generate reliable ground truths on new contrasts, we adopted the already implemented `localizer-based strategy` (see README).

Because the `spinegeneric` and `whole-spine` datasets contain multiple contrasts acquired during the same session, we could leverage the fact that these contrasts are generally well aligned:

1. TotalSpineSeg was first run on the T2w scans (the training contrast, where cervical landmarks are accurately identified)

```
CUDA_VISIBLE_DEVICES=1 totalspineseg t2w out-t2w
```

2. TotalSpineSeg was then run on the other contrasts (e.g. T1w) using the localizer-based implementation:

```
CUDA_VISIBLE_DEVICES=1 totalspineseg t1w out-t1w --loc out-t2w/step2_output/ --suffix _T1w --loc-suffix _T2w
```

> Note: The localizer-based approach enabled us to combine the reliable semantic segmentations on T1w scans with the accurate cervical landmarks from T2w scans. Final predictions were then quality-controlled to ensure the robustness of the process.





